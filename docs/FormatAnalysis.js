importScripts("https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js");

// Global variable to store the format
let appFormat = 'Standard'; // Default value

// Listen for messages from the main thread
self.addEventListener('message', function(e) {
    if (e.data.type === 'init' && e.data.params) {
        // Store the format parameter
        appFormat = e.data.params.format || 'Standard';
        console.log('Received format parameter:', appFormat);
    }
    // Handle other messages...
});

function sendPatch(patch, buffers, msg_id) {
  self.postMessage({
    type: 'patch',
    patch: patch,
    buffers: buffers
  })
}

async function startApplication() {
  console.log("Loading pyodide!");
  self.postMessage({type: 'status', msg: 'Loading pyodide'})
  self.pyodide = await loadPyodide();
  self.pyodide.globals.set("sendPatch", sendPatch);
  console.log("Loaded!");
  await self.pyodide.loadPackage("micropip");
  const env_spec = ['https://cdn.holoviz.org/panel/wheels/bokeh-3.4.3-py3-none-any.whl', 'https://cdn.holoviz.org/panel/1.4.5/dist/wheels/panel-1.4.5-py3-none-any.whl', 'pyodide-http==0.2.1', 'holoviews', 'numpy', 'pandas', 'param', 'requests', 'scipy']
  for (const pkg of env_spec) {
    let pkg_name;
    if (pkg.endsWith('.whl')) {
      pkg_name = pkg.split('/').slice(-1)[0].split('-')[0]
    } else {
      pkg_name = pkg
    }
    self.postMessage({type: 'status', msg: `Installing ${pkg_name}`})
    try {
      await self.pyodide.runPythonAsync(`
        import micropip
        await micropip.install('${pkg}');
      `);
    } catch(e) {
      console.log(e)
      self.postMessage({
	type: 'status',
	msg: `Error while installing ${pkg_name}`
      });
    }
  }
  console.log("Packages loaded!");
  self.postMessage({type: 'status', msg: 'Reading Data'});
  console.log('Starting application with format:', appFormat);
    
  // Fetch the format data we want
  let zipResponse = await fetch(`${appFormat}.zip`);
  let zipBinary = await zipResponse.arrayBuffer();
  self.pyodide.unpackArchive(zipBinary, "zip");
  console.log("Data loaded!");
  self.postMessage({type: 'status', msg: 'Executing code'})
  const code = `
  \nimport asyncio\n\nfrom panel.io.pyodide import init_doc, write_doc\n\ninit_doc()\n\n\nfrom collections import Counter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport holoviews as hv\nimport panel as pn\n\nimport requests\nimport json\n\nimport param\n\npn.extension(\n    'tabulator', \n    sizing_mode="stretch_width", \n    throttled=True, \n    js_files={'hover': 'hover.js'},\n)\nhv.extension('bokeh')\n\nfrom scipy import sparse\nfrom scipy.stats import binomtest\n\n__version__ = 20250308\n\nclass MTGAnalyzer(param.Parameterized):\n    selected_cards = param.List(default=[], doc="Cards required in deck")\n    excluded_cards = param.List(default=[], doc="Cards excluded from deck")\n    cluster_view = param.Boolean(default=True, doc="Show cluster view instead of card presence view")\n    selected_card = param.String(default='', doc="Card to analyze in detail")\n    date_range = param.DateRange(default=None, doc="Date range for analysis")\n    valid_rows = param.Array(default=np.array([]), doc="Selected indices")\n    valid_wr_rows = param.Array(default=np.array([]), doc="Selected indices with valid wr")\n    \n    def __init__(self, df, card_vectors, vocabulary, oracleid_lookup, **params):\n        super().__init__(**params)\n        self.df = df\n        self.X = card_vectors\n        self.feature_names = vocabulary\n        self.oracleid_lookup = oracleid_lookup\n        self._initialize_card_list()\n        self.find_valid_rows()\n        \n    def _initialize_card_list(self):\n        # Get unique cards from feature names, removing _SB suffix\n        self.card_options = sorted(list(set(\n            [name.replace('_SB', '') for name in self.feature_names.keys()]\n        )))\n    \n    @param.depends('date_range', 'selected_cards', 'excluded_cards', watch=True)\n    def find_valid_rows(self):\n        """\n        Find row indices where specified logical combinations of cards are present.\n        """\n\n        row_mask = np.ones(self.X.shape[0], dtype=bool)\n        \n        # We need to handle things a bit awkwardly here to account for cards being in either main or sideboard.\n        # This is because each card when present in the sideboard is stored as {CARD}_SB.\n        #\n\n        for pair_idx, pair in enumerate(\n            [(self.feature_names.get(c), self.feature_names.get(f"{c}_SB")) for c in self.selected_cards]\n            # + [(self.feature_names.get(self.selected_card), self.feature_names.get(f"{self.selected_card}_SB"))]\n        ):\n            # Check pair format\n            if not isinstance(pair, (list, tuple)) or len(pair) != 2:\n                raise ValueError(f"Each pair must be a tuple/list of length 2. Error in pair {pair_idx}: {pair}")\n            \n            col1, col2 = pair\n            pair_rows = set()\n            \n            # Handle col1\n            if col1 is not None:\n                if not isinstance(col1, (int, np.integer)):\n                    raise TypeError(f"Column index must be integer or None. Got {type(col1)} for column 1 in pair {pair_idx}")\n                if col1 < 0 or col1 >= self.X.shape[1]:\n                    raise ValueError(f"Column index {col1} out of bounds for matrix with {self.X.shape[1]} columns")\n                pair_rows.update(self.X.getcol(col1).nonzero()[0])\n                \n            # Handle col2\n            if col2 is not None:\n                if not isinstance(col2, (int, np.integer)):\n                    raise TypeError(f"Column index must be integer or None. Got {type(col2)} for column 2 in pair {pair_idx}")\n                if col2 < 0 or col2 >= self.X.shape[1]:\n                    raise ValueError(f"Column index {col2} out of bounds for matrix with {self.X.shape[1]} columns")\n                pair_rows.update(self.X.getcol(col2).nonzero()[0])\n            \n            # If both columns in a pair are None, skip this pair\n            if col1 is None and col2 is None:\n                continue\n                \n            # Create mask for current pair\n            current_mask = np.zeros(self.X.shape[0], dtype=bool)\n            current_mask[list(pair_rows)] = True\n            \n            # Update overall mask (AND condition)\n            row_mask &= current_mask\n            \n        for pair_idx, pair in enumerate(\n            [(self.feature_names.get(c), self.feature_names.get(f"{c}_SB")) for c in self.excluded_cards]\n        ):\n            # Check pair format\n            if not isinstance(pair, (list, tuple)) or len(pair) != 2:\n                raise ValueError(f"Each pair must be a tuple/list of length 2. Error in pair {pair_idx}: {pair}")\n            \n            col1, col2 = pair\n            pair_rows = set()\n            \n            # Handle col1\n            if col1 is not None:\n                if not isinstance(col1, (int, np.integer)):\n                    raise TypeError(f"Column index must be integer or None. Got {type(col1)} for column 1 in pair {pair_idx}")\n                if col1 < 0 or col1 >= self.X.shape[1]:\n                    raise ValueError(f"Column index {col1} out of bounds for matrix with {self.X.shape[1]} columns")\n                pair_rows.update(self.X.getcol(col1).nonzero()[0])\n                \n            # Handle col2\n            if col2 is not None:\n                if not isinstance(col2, (int, np.integer)):\n                    raise TypeError(f"Column index must be integer or None. Got {type(col2)} for column 2 in pair {pair_idx}")\n                if col2 < 0 or col2 >= self.X.shape[1]:\n                    raise ValueError(f"Column index {col2} out of bounds for matrix with {self.X.shape[1]} columns")\n                pair_rows.update(self.X.getcol(col2).nonzero()[0])\n            \n            # If both columns in a pair are None, skip this pair\n            if col1 is None and col2 is None:\n                continue\n                \n            # Create mask for current pair\n            current_mask = np.zeros(self.X.shape[0], dtype=bool)\n            current_mask[list(pair_rows)] = True\n            \n            # Update overall mask (AND condition)\n            row_mask &= ~current_mask\n\n        # Add time bounds filter\n        # print(self.df.head())\n        # print(self.date_range)\n        if self.date_range:\n            row_mask &= (self.df['Date'] >= self.date_range[0]) & (self.df['Date'] <= self.date_range[1])\n            # if self.date_range[1]:\n            #     row_mask \n        \n        # Return row indices that satisfy all conditions\n        # print(row_mask)\n        self.valid_rows = np.where(row_mask)[0]\n        self.valid_wr_rows = np.where(row_mask & ~self.df['Invalid_WR'])[0]\n        # print(self.valid_wr_rows)\n\n    @param.depends('valid_wr_rows')\n    def get_selection_info(self):\n        return pn.Row(\n            pn.pane.Markdown(\n                f'You have selected {self.valid_rows.shape[0]} decks, {self.valid_wr_rows.shape[0]} of which have valid win rate information.',\n                sizing_mode='stretch_width',\n            ),\n            pn.widgets.TooltipIcon(\n                value="League data and other sources only show decks with 100% winrate, so they can't be included in win rate calculations. They still contribute to aggregation info.",\n                max_width=10\n            ),\n        )\n\n    @param.depends('valid_wr_rows')\n    def get_deck_view(self):\n        valid_cards = np.unique(self.X[self.valid_rows].nonzero()[1])\n        # if valid_cards.shape[0] > 500:\n        #     return pn.pane.Markdown(\n        #         f'''Too many cards to display deck aggregation. Make a more restrictive filter.\n        #         Current cards: {valid_cards.shape[0]}, Max cards: 500\n        #         '''\n        #     )\n        \n        # Work out how many of each card is played in aggregate.\n        #\n        counts_df = pd.DataFrame(\n            sparse_column_value_counts(\n                self.X[self.valid_rows][:, valid_cards]\n            )\n        ).fillna(0)\n\n        # Index properly by card name.\n        #\n        idx_card_map = {v: k for k, v in self.feature_names.items()}\n        counts_df.index = [idx_card_map.get(c) for c in valid_cards]\n        counts_df.index.name = 'Card'\n\n        # Handle for when we have more than 4 of a card.\n        # We should be able to aggregate to 4+ without losing value.\n        # Mono color standard or limited decks are the only real issue here, where basics show up >4x.\n        #\n        if any(counts_df.columns>4):\n            counts_df['4+'] = np.nansum(counts_df[[col for col in counts_df.columns if col>=4]], axis=1)\n            counts_df = counts_df.rename(columns={0:'0',1:'1',2:'2',3:'3'})\n            col_list = ['0','1','2','3','4+']\n            \n        else:\n            counts_df = counts_df.rename(columns={0:'0',1:'1',2:'2',3:'3',4:'4'})\n            col_list = ['0','1','2','3','4']\n\n        counts_df = counts_df[col_list]\n        counts_df.fillna(0)\n\n        # Split into main/sb.\n        #\n        mb_counts_df = counts_df.loc[\n            [i for i in counts_df.index if not i.endswith('_SB')]\n        ].sort_values(\n            col_list\n        )\n\n        sb_counts_df = counts_df.loc[\n            [i for i in counts_df.index if i.endswith('_SB')]\n        ].sort_values(\n            col_list\n        )\n\n        # Remove the _SB suffix\n        #\n        sb_counts_df.index = [c[:-3] for c in sb_counts_df.index]\n        sb_counts_df.index.name = 'Card'\n\n        # Preprocess DataFrame to apply HTML formatter\n        #\n        for col in col_list:\n            mb_counts_df[col] = mb_counts_df[col].apply(vertical_bar_html)\n            sb_counts_df[col] = sb_counts_df[col].apply(vertical_bar_html)\n\n        # Create tabulator with HTML formatter.\n        # First do all of the qtty columns.\n        #\n        formatters = {\n            col: {'type': 'html'} for col in col_list\n        }\n\n        # Then do the name column.\n        formatters['Card'] = {'type': 'html'}\n\n        def card_name_formatter(card_name):\n            # Return HTML with data attribute for the image URL\n            #\n            return f"""<hover-card oracleId="{self.oracleid_lookup.get(card_name)}">{card_name}</hover-card><br>"""\n\n        mb_counts_df = mb_counts_df.reset_index()\n        mb_counts_df['Card'] = mb_counts_df['Card'].apply(card_name_formatter)\n        sb_counts_df = sb_counts_df.reset_index()\n        sb_counts_df['Card'] = sb_counts_df['Card'].apply(card_name_formatter)\n\n        mb_table = pn.widgets.Tabulator(\n            mb_counts_df, \n            formatters=formatters, \n            pagination='local', \n            show_index=False,\n            disabled=True,\n        )\n        sb_table = pn.widgets.Tabulator(\n            sb_counts_df, \n            formatters=formatters, \n            pagination='local', \n            show_index=False,\n            disabled=True,\n        )\n    \n        return pn.Row(\n            pn.Column(\n                pn.pane.HTML('<h3>Main</h3>'),\n                mb_table,\n            ), \n            pn.Column(\n                pn.pane.HTML('<h3>Sideboard</h3>'),\n                sb_table,\n            ),\n        )\n     \n    @param.depends('selected_card', 'valid_rows')\n    def get_card_analysis(self):\n        """Analyse the prevalence of a specific card, quantity distribution."""\n\n        if not self.selected_card:\n            return pn.pane.Markdown("Select a card to see analysis")\n            \n        mb_idx = self.feature_names.get(self.selected_card)\n        sb_idx = self.feature_names.get(f"{self.selected_card}_SB")\n        \n        if (mb_idx is None) and (sb_idx is None):\n            return pn.pane.Markdown("Card not found in dataset")\n        \n        # We need to handle for when the card shows up just in sb/mb/both.\n        #\n        if mb_idx is None:\n            mb_copies = [np.nan]\n            _, _, sb_copies = sparse.find(self.X[self.valid_rows][:, sb_idx])\n            n_decks = sb_copies.shape[0]\n        elif sb_idx is None:\n            sb_copies = [np.nan]\n            _, _, mb_copies = sparse.find(self.X[self.valid_rows][:, mb_idx])\n            n_decks = mb_copies.shape[0]\n        else:\n            mb_d, _ = self.X[self.valid_rows][:, mb_idx].nonzero()\n            sb_d, _ = self.X[self.valid_rows][:, sb_idx].nonzero()\n            d = set(np.concatenate([mb_d, sb_d]))\n            mb_copies = self.X[self.valid_rows][list(d), mb_idx].toarray().flatten()\n            sb_copies = self.X[self.valid_rows][list(d), sb_idx].toarray().flatten()\n            n_decks = len(d)\n\n        bins = np.arange(-0.5, np.nanmax([np.nanmax(mb_copies), np.nanmax(sb_copies), 5]), 1)\n        # mb_y, _ = np.histogram(mb_copies, bins, density=True)\n        # sb_y, _ = np.histogram(sb_copies, bins, density=True)\n\n        mb_y, _ = np.histogram(mb_copies, bins)\n        mb_y[0] += self.valid_rows.shape[0] - n_decks\n        mb_y = mb_y / mb_y.sum()\n\n        sb_y, _ = np.histogram(sb_copies, bins)\n        sb_y[0] += self.valid_rows.shape[0] - n_decks\n        sb_y = sb_y / sb_y.sum()\n\n        return hv.Bars(\n            pd.DataFrame({\n                'Frequency': np.concatenate([mb_y, sb_y]),\n                'Qtty': [0,1,2,3,4,0,1,2,3,4],\n                'Board': ['M']*5 + ['SB'] * 5,\n                # 'B': ['Main'] * 5 + ['Sideboard'] * 5\n            }),\n            kdims=['Qtty', 'Board'],\n        ).opts(\n            width=400,\n            height=400,\n            title=f"Qtty Frequency",\n            toolbar=None, \n            default_tools=[],\n            active_tools=[],\n        )\n    \n    @param.depends('selected_card', 'valid_wr_rows')\n    def get_winrate_analysis(self):\n        """Perform card quantity based win rate analysis.\n        Error bars are confidence interval on the binomial test.\n        """\n\n        if not self.selected_card:\n            return pn.pane.Markdown("Select a card to see win rate analysis")\n            \n        # Calculate win rates by copy count\n        \n        if not self.selected_card in self.feature_names:\n            return pn.pane.Markdown("Card not found in dataset")\n        \n        plots = list()\n\n        # Handle differently if the card doesn't show up in main/side.\n        #\n        \n        mb_idx = self.feature_names.get(self.selected_card)\n        if mb_idx is not None:    \n            copy_counts = self.X[self.valid_wr_rows][:, mb_idx].toarray()\n\n            # print(self.df.loc[self.valid_wr_rows,['Wins']].value_counts(), copy_counts)\n            \n            mb_win_rates = []\n            for i in range(5):  # 0-4 copies\n                mask = copy_counts == i\n                wins = self.df.loc[self.valid_wr_rows].reset_index().loc[mask.ravel(), 'Wins'].sum()\n                total = wins + self.df.loc[self.valid_wr_rows].reset_index().loc[mask.ravel(), 'Losses'].sum()\n                if total:\n                    ci = binomtest(k=int(wins), n=int(total)).proportion_ci()\n                    winrate = wins/total if total else np.nan\n                    mb_win_rates.append({\n                        'copies': i-0.1, \n                        'winrate': winrate,\n                        'errmin': winrate - ci.low,\n                        'errmax': ci.high - winrate,\n                    })\n                else:\n                    # For completeness\n                    #\n                    mb_win_rates.append({\n                        'copies': i-0.1, \n                        'winrate': np.nan,\n                        'errmin': np.nan,\n                        'errmax': np.nan,\n                    })\n            \n            plots.append(hv.Scatter(\n                mb_win_rates, 'copies', 'winrate', label='Main',\n            ).opts(size=7,))\n            plots.append(hv.ErrorBars(\n                mb_win_rates, 'copies', vdims=['winrate', 'errmin', 'errmax'],\n            ))\n\n        sb_idx = self.feature_names.get(f'{self.selected_card}_SB')\n        if sb_idx is not None:    \n            copy_counts = self.X[self.valid_wr_rows][:, sb_idx].toarray()\n            \n            sb_win_rates = []\n            for i in range(5):  # 0-4 copies\n                mask = copy_counts == i\n                wins = self.df.loc[self.valid_wr_rows].reset_index().loc[mask.ravel(), 'Wins'].sum()\n                total = wins + self.df.loc[self.valid_wr_rows].reset_index().loc[mask.ravel(), 'Losses'].sum()\n\n                if total:\n                    ci = binomtest(k=int(wins), n=int(total)).proportion_ci()\n                    winrate = wins/total if total else np.nan\n                    sb_win_rates.append({\n                        'copies': i+0.1, \n                        'winrate': winrate,\n                        'errmin': winrate - ci.low,\n                        'errmax': ci.high - winrate,\n                    })\n                else:\n                    # For completeness\n                    #\n                    sb_win_rates.append({\n                        'copies': i+0.1, \n                        'winrate': np.nan,\n                        'errmin': np.nan,\n                        'errmax': np.nan,\n                    })\n            \n            plots.append(hv.Scatter(\n                sb_win_rates, 'copies', 'winrate', label='Sideboard',\n            ).opts(size=7, toolbar=None, default_tools=[],))\n            plots.append(hv.ErrorBars(\n                sb_win_rates, 'copies', vdims=['winrate', 'errmin', 'errmax'],\n            ).opts(toolbar=None, default_tools=[],))\n\n        # Add helper lines for context, 50% and the average wr of selected decks.\n        #\n        wins = self.df.loc[self.valid_wr_rows]['Wins'].sum()\n        total = wins + self.df.loc[self.valid_wr_rows]['Losses'].sum()\n        wr = wins/total\n        # return hv.Curve([(0.5, 0.5),(5.5, 0.5)], 'copies', label='50% wr').opts(color='k', line_dash='dotted')\n\n        plots.extend([\n            hv.Curve([(-0.5, 0.5),(4.5, 0.5)], 'copies', 'winrate', label='50% wr').opts(\n                color='k', \n                line_dash='dotted',\n                toolbar=None, \n                default_tools=[],\n            ),\n            hv.Curve([(-0.5, wr),(4.5, wr)], 'copies', 'winrate', label='Deck average').opts(\n                color='k', \n                line_dash='dashed',\n                toolbar=None, \n                default_tools=[],\n            )\n        ])\n                \n        # Create line plot using HoloViews\n        win_rate_plot = hv.Overlay(plots).opts(\n            width=400,\n            height=400,\n            title=f"Win Rate by Copy Count",\n            ylabel='Win Rate',\n            xlabel='Number of Copies',\n            xlim=(-0.5, 4.5),\n            ylim=(-0.1, 1.1),\n            toolbar=None,\n            default_tools=[],\n            active_tools=[],\n            legend_position='bottom_left',\n            legend_cols=4,\n        )\n        \n        return win_rate_plot\n\n\n# Load and process data\ndef load_data(data_path='processed_data', lookback_days=365):\n    """\n    Load preprocessed MTG tournament data for dashboard visualization.\n    \n    Parameters:\n    -----------\n    data_path : str\n        Path to the directory containing processed data files\n    lookback_days : int\n        Number of days of data to load (to avoid loading entire history)\n        \n    Returns:\n    --------\n    tuple\n        (\n            DataFrame with deck data, \n            sparse matrix of card counts, \n            fitted CountVectorizer vocabulary,\n            dictionary for oracleid lookups,\n        )\n    """\n    # Load the preprocessed data\n    with open(Path(data_path) / 'deck_data.json', 'r') as f:\n        data = json.load(f)\n        \n    # Convert to DataFrame\n    df = pd.DataFrame(data['decks'])\n    df['Date'] = pd.to_datetime(df['Date']).dt.date\n    \n    # Load cluster labels\n    # df['Cluster'] = data['clusters']\n\n    # Filter to recent data\n    cutoff_date = (pd.to_datetime('today') - pd.Timedelta(days=lookback_days)).date()\n\n    # Load card vectors\n    X = sparse.load_npz(Path(data_path) / 'card_vectors.npz')[df['Date'] >= cutoff_date]\n\n    df = df[df['Date'] >= cutoff_date].reset_index()\n    \n    # Load and reconstruct vectorizer\n    with open(Path(data_path) / 'vectorizer.json', 'r') as f:\n        vectorizer_data = json.load(f)\n\n    # Load oracleid lookup\n    with open(Path(data_path) / 'card_data.json', 'r') as f:\n        oracleid_lookup = json.load(f)\n    \n    # vectorizer = CountVectorizer()\n    # vectorizer.vocabulary_ = vectorizer_data['vocabulary']\n    # vectorizer.fixed_vocabulary_ = True\n    \n    return df, X, vectorizer_data['vocabulary'], oracleid_lookup\n\ndef sparse_column_value_counts(sparse_matrix, normalize=True):\n    """\n    Calculate value counts for each column in a sparse matrix without densification.\n    \n    Parameters:\n    -----------\n    sparse_matrix : scipy.sparse.spmatrix\n        Input sparse matrix (will be converted to CSC format internally)\n    normalize : bool, default=True\n        If True, returns the relative frequency of values. If False, returns counts.\n    \n    Returns:\n    --------\n    list of dicts\n        Each dict contains value:count pairs for a column.\n        If normalize=True, counts are replaced with frequencies.\n    """\n    # Convert to CSC for efficient column access\n    #\n    if not sparse.isspmatrix_csc(sparse_matrix):\n        csc_matrix = sparse_matrix.tocsc()\n    else:\n        csc_matrix = sparse_matrix\n    \n    n_rows, n_cols = csc_matrix.shape\n    result = []\n    \n    for col_idx in range(n_cols):\n        # Get column data and row indices\n        #\n        start = csc_matrix.indptr[col_idx]\n        end = csc_matrix.indptr[col_idx + 1]\n        data = csc_matrix.data[start:end]\n        \n        # Count explicitly stored values\n        #\n        counter = Counter(data)\n        \n        # Add count for zeros (elements not explicitly stored)\n        #\n        explicit_entries = end - start\n        zeros_count = n_rows - explicit_entries\n        if zeros_count > 0:\n            counter[0] = zeros_count\n        \n        # Normalize if requested\n        #\n        if normalize:\n            total = n_rows\n            counter = {k: v / total for k, v in counter.items()}\n        \n        result.append(counter)\n    \n    return result\n\ndef vertical_bar_html(value):\n    """\n    Format a tabulator with a vertical bar to produce histograms across neighbouring columns.\n    Input should already be normalized to between 0,1.\n    """\n    if pd.isna(value):\n        return ""\n    \n    percent = max(0, min(100, value * 100))\n    \n    return f"""\n        <div style="margin: 0 auto; position: relative; width: 30px; height: 20px; background-color: #f0f0f0; border-radius: 3px;">\n            <div style="position: absolute; bottom: 0; left: 0; width: 100%; height: {percent}%; background-color: #6495ED; border-radius: 0 0 3px 3px;"></div>\n            <div style="position: absolute; width: 100%; text-align: center; top: 50%; transform: translateY(-50%); font-size: 10px;">{percent:.0f}%</div>\n        </div>\n    """\n\n# Create the dashboard\n#\ndef create_dashboard(df, X, vocabulary, oracleid_lookup):\n    analyzer = MTGAnalyzer(df, X, vocabulary, oracleid_lookup)\n    \n    # Create card selection widget\n    #\n    card_select = pn.widgets.MultiChoice(\n        name='Required Cards',\n        options=analyzer.card_options,\n        value=[],\n        placeholder='Search for cards...',\n        sizing_mode='stretch_width'\n    )\n\n    # Create card selection widget\n    #\n    card_exclude = pn.widgets.MultiChoice(\n        name='Excluded Cards',\n        options=analyzer.card_options,\n        value=[],\n        placeholder='Search for cards...',\n        sizing_mode='stretch_width'\n    )\n    \n    # Create date range selector\n    #\n    date_range = pn.widgets.DateRangeSlider(\n        name='Date Range',\n        start=df['Date'].min(),\n        end=df['Date'].max(),\n        value=(df['Date'].max() - pd.Timedelta(weeks=3*4), df['Date'].max()),\n        sizing_mode='stretch_width'\n    )\n    \n    # Create card analysis widgets\n    #\n    card_analysis = pn.widgets.Select(\n        name='Analyze Card',\n        options=[''] + analyzer.card_options,\n        sizing_mode='stretch_width'\n    )\n    \n    # Link widgets to analyzer parameters\n    #\n    card_select.link(analyzer, value='selected_cards')\n    card_exclude.link(analyzer, value='excluded_cards')\n    card_analysis.link(analyzer, value='selected_card')\n    date_range.link(analyzer, value='date_range')\n\n    description = pn.pane.HTML(\n        '''\n        Urza's Research Desk brought to you by me, <a target="_blank" rel="noopener noreferrer" href="https://bsky.app/profile/arckaynine.bsky.social">ArcKayNine</a>.<br>\n        All data comes courtesy of the excellent work done by <a target="_blank" rel="noopener noreferrer" href="https://github.com/Badaro/MTGODecklistCache">Badaro</a>.<br>      \n        For more of my work, check out my blog, <a target="_blank" rel="noopener noreferrer" href="https://compulsiveresearchmtg.blogspot.com">CompulsiveResearchMtg</a> or the exploits of my team, <a href="https://bsky.app/profile/busstop-mtg.bsky.social">Team Bus Stop</a>.<br>\n        If you find this useful, valuable, or interesting, consider supporting further work via my <a target="_blank" rel="noopener noreferrer" href="https://ko-fi.com/arckaynine">Ko-fi</a>.<br>\n        Urza's Research Desk is unofficial Fan Content permitted under the Fan Content Policy. Not approved/endorsed by Wizards. Portions of the materials used are property of Wizards of the Coast. \xa9Wizards of the Coast LLC.<br>\n        ''',\n    )\n    \n    # Create layout groups\n    #\n    controls = pn.Column(\n        pn.pane.Markdown("## MTG Deck Analysis"),\n        pn.pane.Markdown("To filter down the decks you're looking at, select cards that are required in the 75, and cards that cannot be in the 75."),\n        card_select,\n        card_exclude,\n        date_range,\n        analyzer.get_selection_info,\n        description,\n        sizing_mode='stretch_width'\n    )\n    \n    aggregate_view = pn.Column(\n        analyzer.get_deck_view,\n        sizing_mode='stretch_both',\n        name='Aggregate Deck Analysis'\n    )\n    \n    analysis_view = pn.Column(\n        card_analysis,\n        pn.Row(\n            analyzer.get_card_analysis,\n            analyzer.get_winrate_analysis,\n        ),\n        sizing_mode='stretch_both',\n        name='Card Performance Analysis'\n    )\n    \n    # Create template\n    #\n    template = pn.template.FastListTemplate(\n        title="Urza's Research Desk",\n        sidebar=[controls],\n        main=[\n            pn.Tabs(\n                aggregate_view,\n                analysis_view,\n                # TODO\n                # Temporal analysis (moving average population + wr)\n                sizing_mode='stretch_width',\n                dynamic=True, # Only render the active tab.\n            ),\n        ],\n    )\n    \n    return template\n\n\n# if __name__ == '__main__':\ndf, X, vocabulary, oracleid_lookup = load_data()\ndashboard = create_dashboard(df, X, vocabulary, oracleid_lookup)\ndashboard.servable()\n\nawait write_doc()
  `

  try {
    const [docs_json, render_items, root_ids] = await self.pyodide.runPythonAsync(code)
    self.postMessage({
      type: 'render',
      docs_json: docs_json,
      render_items: render_items,
      root_ids: root_ids
    })
  } catch(e) {
    const traceback = `${e}`
    const tblines = traceback.split('\n')
    self.postMessage({
      type: 'status',
      msg: tblines[tblines.length-2]
    });
    throw e
  }
}

self.onmessage = async (event) => {
  const msg = event.data
  if (msg.type === 'rendered') {
    self.pyodide.runPythonAsync(`
    from panel.io.state import state
    from panel.io.pyodide import _link_docs_worker

    _link_docs_worker(state.curdoc, sendPatch, setter='js')
    `)
  } else if (msg.type === 'patch') {
    self.pyodide.globals.set('patch', msg.patch)
    self.pyodide.runPythonAsync(`
    from panel.io.pyodide import _convert_json_patch
    state.curdoc.apply_json_patch(_convert_json_patch(patch), setter='js')
    `)
    self.postMessage({type: 'idle'})
  } else if (msg.type === 'location') {
    self.pyodide.globals.set('location', msg.location)
    self.pyodide.runPythonAsync(`
    import json
    from panel.io.state import state
    from panel.util import edit_readonly
    if state.location:
        loc_data = json.loads(location)
        with edit_readonly(state.location):
            state.location.param.update({
                k: v for k, v in loc_data.items() if k in state.location.param
            })
    `)
  }
}

startApplication()